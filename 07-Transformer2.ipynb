{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tgcAxZkhmiXj"
      },
      "source": [
        "# Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9yFD1SyVwM-q",
        "outputId": "cbd8c92b-c57b-48f9-eba2-bf24e6a82cfb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.3.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.17.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (17.0.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.10.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.12)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.28.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading datasets-3.3.0-py3-none-any.whl (484 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m484.9/484.9 kB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, dill, multiprocess, datasets\n",
            "Successfully installed datasets-3.3.0 dill-0.3.8 multiprocess-0.70.16 xxhash-3.5.0\n",
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2025.1.31)\n",
            "Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.9.0\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
        "!pip install tqdm\n",
        "!pip install datasets\n",
        "# !pip install hellaswag\n",
        "!pip install tiktoken\n",
        "from torch.optim.lr_scheduler import LambdaLR\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import numpy as np\n",
        "import time\n",
        "from torch.amp import autocast, GradScaler\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import os\n",
        "import multiprocessing as mp\n",
        "from datasets import load_dataset\n",
        "from transformers import ElectraTokenizer\n",
        "import glob\n",
        "import shutil\n",
        "import tiktoken\n",
        "import math\n",
        "import inspect\n",
        "from dataclasses import dataclass\n",
        "# from hellaswag import render_example, iterate_examples\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VYNfrDwvXKFY"
      },
      "source": [
        "# Prepare Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "2KTdgnJm_n6u"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tiktoken\n",
        "from datasets import load_dataset\n",
        "from tqdm import tqdm\n",
        "import multiprocessing as mp\n",
        "\n",
        "local_dir = \"/content/edu_fineweb10B\"\n",
        "remote_name = \"sample-10BT\"\n",
        "shard_size = int(1e8)\n",
        "\n",
        "DATA_CACHE_DIR = os.path.join(local_dir)\n",
        "os.makedirs(DATA_CACHE_DIR, exist_ok=True)\n",
        "\n",
        "\n",
        "enc = tiktoken.get_encoding(\"gpt2\")\n",
        "eot = enc._special_tokens['<|endoftext|>']\n",
        "def tokenize(doc):\n",
        "    tokens = [eot]\n",
        "    tokens.extend(enc.encode_ordinary(doc[\"text\"]))\n",
        "    tokens_np = np.array(tokens)\n",
        "    assert (0 <= tokens_np).all() and (tokens_np < 2**16).all(), \"token dictionary too large for uint16\"\n",
        "    tokens_np_uint16 = tokens_np.astype(np.uint16)\n",
        "    return tokens_np_uint16\n",
        "\n",
        "def write_datafile(filename, tokens_np):\n",
        "    np.save(filename, tokens_np)\n",
        "\n",
        "def prepare_fineweb():\n",
        "    fw = load_dataset(\"HuggingFaceFW/fineweb-edu\", name=remote_name, split=\"train\", streaming=True)\n",
        "    nprocs = max(1, os.cpu_count()//2)\n",
        "    with mp.Pool(nprocs) as pool:\n",
        "        shard_index = 0\n",
        "        all_tokens_np = np.empty((shard_size,), dtype=np.uint16)\n",
        "        token_count = 0\n",
        "        progress_bar = None\n",
        "        for tokens in pool.imap(tokenize, fw, chunksize=16):\n",
        "\n",
        "            if token_count + len(tokens) < shard_size:\n",
        "                all_tokens_np[token_count:token_count+len(tokens)] = tokens\n",
        "                token_count += len(tokens)\n",
        "                if progress_bar is None:\n",
        "                    progress_bar = tqdm(total=shard_size, unit=\"tokens\", desc=f\"Shard {shard_index}\")\n",
        "                # progress_bar.update(len(tokens))\n",
        "            else:\n",
        "                split = \"val\" if shard_index == 0 else \"train\"\n",
        "                filename = os.path.join(DATA_CACHE_DIR, f\"edufineweb_{split}_{shard_index:06d}\")\n",
        "                remainder = shard_size - token_count\n",
        "                progress_bar.update(remainder)\n",
        "                all_tokens_np[token_count:token_count+remainder] = tokens[:remainder]\n",
        "                write_datafile(filename, all_tokens_np)\n",
        "                shard_index += 1\n",
        "                progress_bar = None\n",
        "                all_tokens_np[0:len(tokens)-remainder] = tokens[remainder:]\n",
        "                token_count = len(tokens)-remainder\n",
        "\n",
        "        if token_count != 0:\n",
        "            split = \"val\" if shard_index == 0 else \"train\"\n",
        "            filename = os.path.join(DATA_CACHE_DIR, f\"edufineweb_{split}_{shard_index:06d}\")\n",
        "            write_datafile(filename, all_tokens_np[:token_count])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mbOaDKqSLNBM",
        "outputId": "19ac18f2-ee33-4e7e-ae6a-8759f22e851c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from concurrent.futures import ThreadPoolExecutor\n",
        "drive.mount('/content/drive')\n",
        "def copy_to_drive():\n",
        "  google_drive_dir = \"/content/drive/My Drive/datasets/fineweb10B/\"\n",
        "  os.makedirs(google_drive_dir, exist_ok=True)\n",
        "  local_output_dir = \"/content/edu_fineweb10B\"\n",
        "\n",
        "  for filename in os.listdir(local_output_dir):\n",
        "    local_file_path = os.path.join(local_output_dir, filename)\n",
        "    google_drive_file_path = os.path.join(google_drive_dir, filename)\n",
        "    shutil.copy(local_file_path, google_drive_file_path)\n",
        "    print(f\"Copied {filename} to Google Drive.\")\n",
        "\n",
        "  print(f\"All files copied to Google Drive at {google_drive_dir}\")\n",
        "\n",
        "def copy_from_drive():\n",
        "  google_drive_dir = \"/content/drive/My Drive/datasets/fineweb10B/\"\n",
        "  os.makedirs(google_drive_dir, exist_ok=True)\n",
        "  local_output_dir = \"/content/edu_fineweb10B\"\n",
        "  files = os.listdir(google_drive_dir)\n",
        "\n",
        "  for filename in files:\n",
        "    local_file_path = os.path.join(local_output_dir, filename)\n",
        "    google_drive_file_path = os.path.join(google_drive_dir, filename)\n",
        "    shutil.copy(google_drive_file_path, local_file_path)\n",
        "    print(f\"Copied {filename} to Disk.\")\n",
        "\n",
        "  print(f\"All files copied to Disk at {google_drive_dir}\")\n",
        "\n",
        "# shuffle shards\n",
        "import re\n",
        "def shuffle_shards(output_dir):\n",
        "    shard_files = [f for f in os.listdir(output_dir) if f.startswith(\"edufineweb_train_\")]\n",
        "\n",
        "    random.shuffle(shard_files)\n",
        "\n",
        "    for idx, old_filename in enumerate(shard_files):\n",
        "        new_filename = f\"edufineweb_train_{idx:04d}.bin\"\n",
        "\n",
        "        old_filepath = os.path.join(output_dir, old_filename)\n",
        "        new_filepath = os.path.join(output_dir, new_filename)\n",
        "\n",
        "        temp_filepath = os.path.join(output_dir, f\"temp_{old_filename}\")\n",
        "\n",
        "        shutil.move(old_filepath, temp_filepath)\n",
        "\n",
        "        shutil.move(new_filepath, old_filepath) if os.path.exists(new_filepath) else None\n",
        "\n",
        "        shutil.move(temp_filepath, new_filepath)\n",
        "\n",
        "    print(f\"Shuffling complete. Shards renamed and shuffled in {output_dir}\")\n",
        "\n",
        "def load_tokens(filename):\n",
        "    npt = np.load(filename)\n",
        "    npt = npt.astype(np.int32)\n",
        "    ptt = torch.tensor(npt, dtype=torch.long)\n",
        "    return ptt\n",
        "\n",
        "class DataLoaderLite:\n",
        "    def __init__(self, B, T, process_rank, num_processes, split):\n",
        "        self.B = B\n",
        "        self.T = T\n",
        "        self.process_rank = process_rank\n",
        "        self.num_processes = num_processes\n",
        "        assert split in {'train', 'val'}\n",
        "\n",
        "        data_root = \"/content/edu_fineweb10B\"\n",
        "        shards = os.listdir(data_root)\n",
        "        shards = [s for s in shards if split in s]\n",
        "        shards = sorted(shards)\n",
        "        shards = [os.path.join(data_root, s) for s in shards]\n",
        "        self.shards = shards\n",
        "        assert len(shards) > 0, f\"no shards found for split {split}\"\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.current_shard = 0\n",
        "        self.tokens = load_tokens(self.shards[self.current_shard])\n",
        "        self.current_position = self.B * self.T * self.process_rank\n",
        "\n",
        "    def next_batch(self):\n",
        "        B, T = self.B, self.T\n",
        "        buf = self.tokens[self.current_position : self.current_position+B*T+1]\n",
        "        x = (buf[:-1]).view(B, T)\n",
        "        y = (buf[1:]).view(B, T)\n",
        "\n",
        "        self.current_position += B * T * self.num_processes\n",
        "\n",
        "        if self.current_position + (B * T * self.num_processes + 1) > len(self.tokens):\n",
        "            self.current_shard = (self.current_shard + 1) % len(self.shards)\n",
        "            self.tokens = load_tokens(self.shards[self.current_shard])\n",
        "            self.current_position = B * T * self.process_rank\n",
        "        return x, y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8vgyE77KyqGO"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "P7oBWAkay0nt"
      },
      "outputs": [],
      "source": [
        "class CausalSelfAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.n_embd % config.n_head == 0\n",
        "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
        "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
        "        self.c_proj.NANOGPT_SCALE_INIT = 1\n",
        "        self.n_head = config.n_head\n",
        "        self.n_embd = config.n_embd\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.size()\n",
        "        qkv = self.c_attn(x)\n",
        "        q, k, v = qkv.split(self.n_embd, dim=2)\n",
        "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "        y = F.scaled_dot_product_attention(q, k, v, is_causal=True)\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
        "        y = self.c_proj(y)\n",
        "        return y\n",
        "\n",
        "class MLP(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd)\n",
        "        self.gelu    = nn.GELU(approximate='tanh')\n",
        "        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd)\n",
        "        self.c_proj.NANOGPT_SCALE_INIT = 1\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.c_fc(x)\n",
        "        x = self.gelu(x)\n",
        "        x = self.c_proj(x)\n",
        "        return x\n",
        "\n",
        "class Block(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
        "        self.attn = CausalSelfAttention(config)\n",
        "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
        "        self.mlp = MLP(config)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.ln_1(x))\n",
        "        x = x + self.mlp(self.ln_2(x))\n",
        "        return x\n",
        "\n",
        "@dataclass\n",
        "class GPTConfig:\n",
        "    block_size: int = 1024\n",
        "    vocab_size: int = 50304\n",
        "    n_layer: int = 12\n",
        "    n_head: int = 12\n",
        "    n_embd: int = 768\n",
        "\n",
        "class GPT(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        self.transformer = nn.ModuleDict(dict(\n",
        "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
        "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
        "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
        "            ln_f = nn.LayerNorm(config.n_embd),\n",
        "        ))\n",
        "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
        "\n",
        "        self.transformer.wte.weight = self.lm_head.weight\n",
        "\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            std = 0.02\n",
        "            if hasattr(module, 'NANOGPT_SCALE_INIT'):\n",
        "                std *= (2 * self.config.n_layer) ** -0.5\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=std)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.size()\n",
        "        assert T <= self.config.block_size, f\"Cannot forward sequence of length {T}, block size is only {self.config.block_size}\"\n",
        "\n",
        "        pos = torch.arange(0, T, dtype=torch.long, device=idx.device)\n",
        "        pos_emb = self.transformer.wpe(pos)\n",
        "        tok_emb = self.transformer.wte(idx)\n",
        "        x = tok_emb + pos_emb\n",
        "\n",
        "        for block in self.transformer.h:\n",
        "            x = block(x)\n",
        "\n",
        "        x = self.transformer.ln_f(x)\n",
        "        logits = self.lm_head(x)\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
        "        return logits, loss\n",
        "\n",
        "    def configure_optimizers(self, weight_decay, learning_rate, device_type):\n",
        "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
        "        param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n",
        "        decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
        "        nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
        "        optim_groups = [\n",
        "            {'params': decay_params, 'weight_decay': weight_decay},\n",
        "            {'params': nodecay_params, 'weight_decay': 0.0}\n",
        "        ]\n",
        "        num_decay_params = sum(p.numel() for p in decay_params)\n",
        "        num_nodecay_params = sum(p.numel() for p in nodecay_params)\n",
        "        if master_process:\n",
        "            print(f\"num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters\")\n",
        "            print(f\"num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params:,} parameters\")\n",
        "        fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n",
        "        use_fused = fused_available and device_type == \"cuda\"\n",
        "        if master_process:\n",
        "            print(f\"using fused AdamW: {use_fused}\")\n",
        "        optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=(0.9, 0.95), eps=1e-8, fused=use_fused)\n",
        "        return optimizer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dIM1aHKmHR8H"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "hiNp1blGHQU9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80f55d1e-9e3c-4291-cbb6-f9906929e4ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copied edufineweb_train_000009.npy to Disk.\n",
            "Copied edufineweb_train_000023.npy to Disk.\n",
            "Copied edufineweb_train_000042.npy to Disk.\n",
            "Copied edufineweb_train_000064.npy to Disk.\n",
            "Copied edufineweb_train_000041.npy to Disk.\n",
            "Copied edufineweb_train_000096.npy to Disk.\n",
            "Copied edufineweb_train_000005.npy to Disk.\n",
            "Copied edufineweb_train_000083.npy to Disk.\n",
            "Copied edufineweb_train_000068.npy to Disk.\n",
            "Copied edufineweb_train_000094.npy to Disk.\n",
            "Copied edufineweb_train_000084.npy to Disk.\n",
            "Copied edufineweb_train_000051.npy to Disk.\n",
            "Copied edufineweb_train_000088.npy to Disk.\n",
            "Copied edufineweb_train_000024.npy to Disk.\n",
            "Copied edufineweb_train_000095.npy to Disk.\n",
            "Copied edufineweb_train_000047.npy to Disk.\n",
            "Copied edufineweb_train_000015.npy to Disk.\n",
            "Copied edufineweb_train_000076.npy to Disk.\n",
            "Copied edufineweb_train_000007.npy to Disk.\n",
            "Copied edufineweb_train_000080.npy to Disk.\n",
            "Copied edufineweb_train_000053.npy to Disk.\n",
            "Copied edufineweb_train_000062.npy to Disk.\n",
            "Copied edufineweb_train_000060.npy to Disk.\n",
            "Copied edufineweb_train_000018.npy to Disk.\n",
            "Copied edufineweb_train_000050.npy to Disk.\n",
            "Copied edufineweb_train_000034.npy to Disk.\n",
            "Copied edufineweb_train_000016.npy to Disk.\n",
            "Copied edufineweb_train_000092.npy to Disk.\n",
            "Copied edufineweb_train_000022.npy to Disk.\n",
            "Copied edufineweb_train_000011.npy to Disk.\n",
            "Copied edufineweb_train_000079.npy to Disk.\n",
            "Copied edufineweb_train_000004.npy to Disk.\n",
            "Copied edufineweb_train_000070.npy to Disk.\n",
            "Copied edufineweb_train_000099.npy to Disk.\n",
            "Copied edufineweb_train_000020.npy to Disk.\n",
            "Copied edufineweb_train_000058.npy to Disk.\n",
            "Copied edufineweb_train_000078.npy to Disk.\n",
            "Copied edufineweb_train_000086.npy to Disk.\n",
            "Copied edufineweb_train_000067.npy to Disk.\n",
            "Copied edufineweb_train_000090.npy to Disk.\n",
            "Copied edufineweb_train_000071.npy to Disk.\n",
            "Copied edufineweb_train_000025.npy to Disk.\n",
            "Copied edufineweb_train_000082.npy to Disk.\n",
            "Copied edufineweb_train_000075.npy to Disk.\n",
            "Copied edufineweb_train_000074.npy to Disk.\n",
            "Copied edufineweb_train_000072.npy to Disk.\n",
            "Copied edufineweb_train_000003.npy to Disk.\n",
            "Copied edufineweb_train_000044.npy to Disk.\n",
            "Copied edufineweb_train_000006.npy to Disk.\n",
            "Copied edufineweb_train_000031.npy to Disk.\n",
            "Copied edufineweb_train_000063.npy to Disk.\n",
            "Copied edufineweb_train_000059.npy to Disk.\n",
            "Copied edufineweb_train_000073.npy to Disk.\n",
            "Copied edufineweb_train_000010.npy to Disk.\n",
            "Copied edufineweb_train_000055.npy to Disk.\n",
            "Copied edufineweb_train_000077.npy to Disk.\n",
            "Copied edufineweb_train_000057.npy to Disk.\n",
            "Copied edufineweb_train_000001.npy to Disk.\n",
            "Copied edufineweb_train_000091.npy to Disk.\n",
            "Copied edufineweb_train_000045.npy to Disk.\n",
            "Copied edufineweb_train_000069.npy to Disk.\n",
            "Copied edufineweb_train_000026.npy to Disk.\n",
            "Copied edufineweb_train_000085.npy to Disk.\n",
            "Copied edufineweb_train_000061.npy to Disk.\n",
            "Copied edufineweb_train_000012.npy to Disk.\n",
            "Copied edufineweb_train_000029.npy to Disk.\n",
            "Copied edufineweb_train_000043.npy to Disk.\n",
            "Copied edufineweb_train_000093.npy to Disk.\n",
            "Copied edufineweb_train_000046.npy to Disk.\n",
            "Copied edufineweb_train_000002.npy to Disk.\n",
            "Copied edufineweb_train_000033.npy to Disk.\n",
            "Copied edufineweb_val_000000.npy to Disk.\n",
            "Copied edufineweb_train_000097.npy to Disk.\n",
            "Copied edufineweb_train_000056.npy to Disk.\n",
            "Copied edufineweb_train_000019.npy to Disk.\n",
            "Copied edufineweb_train_000048.npy to Disk.\n",
            "Copied edufineweb_train_000089.npy to Disk.\n",
            "Copied edufineweb_train_000036.npy to Disk.\n",
            "Copied edufineweb_train_000035.npy to Disk.\n",
            "Copied edufineweb_train_000014.npy to Disk.\n",
            "Copied edufineweb_train_000065.npy to Disk.\n",
            "Copied edufineweb_train_000017.npy to Disk.\n",
            "Copied edufineweb_train_000066.npy to Disk.\n",
            "Copied edufineweb_train_000008.npy to Disk.\n",
            "Copied edufineweb_train_000054.npy to Disk.\n",
            "Copied edufineweb_train_000039.npy to Disk.\n",
            "Copied edufineweb_train_000028.npy to Disk.\n",
            "Copied edufineweb_train_000027.npy to Disk.\n",
            "Copied edufineweb_train_000049.npy to Disk.\n",
            "Copied edufineweb_train_000037.npy to Disk.\n",
            "Copied edufineweb_train_000021.npy to Disk.\n",
            "Copied edufineweb_train_000087.npy to Disk.\n",
            "Copied edufineweb_train_000052.npy to Disk.\n",
            "Copied edufineweb_train_000081.npy to Disk.\n",
            "Copied edufineweb_train_000098.npy to Disk.\n",
            "Copied edufineweb_train_000030.npy to Disk.\n",
            "Copied edufineweb_train_000040.npy to Disk.\n",
            "Copied edufineweb_train_000038.npy to Disk.\n",
            "Copied edufineweb_train_000032.npy to Disk.\n",
            "Copied edufineweb_train_000013.npy to Disk.\n",
            "All files copied to Disk at /content/drive/My Drive/datasets/fineweb10B/\n"
          ]
        }
      ],
      "source": [
        "# prepare_fineweb()\n",
        "# copy_to_drive()\n",
        "copy_from_drive()\n",
        "# shuffle_shards(\"/content/edu_fineweb10B\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qbbVWJVfystq"
      },
      "source": [
        "# Model Initialisation and training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KLEN3tRMywOR",
        "outputId": "51770dab-0a06-47e6-eec0-b919abf8843c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "using device: cuda\n",
            "total desired batch size: 524288\n",
            "=> calculated gradient accumulation steps: 16\n"
          ]
        }
      ],
      "source": [
        "from torch.distributed import init_process_group, destroy_process_group\n",
        "from torch.nn.parallel import DistributedDataParallel as DDP\n",
        "import torch.distributed as dist\n",
        "\n",
        "ddp = int(os.environ.get('RANK', -1)) != -1\n",
        "if ddp:\n",
        "    assert torch.cuda.is_available(), \"for now i think we need CUDA for DDP\"\n",
        "    init_process_group(backend='nccl')\n",
        "    ddp_rank = int(os.environ['RANK'])\n",
        "    ddp_local_rank = int(os.environ['LOCAL_RANK'])\n",
        "    ddp_world_size = int(os.environ['WORLD_SIZE'])\n",
        "    device = f'cuda:{ddp_local_rank}'\n",
        "    torch.cuda.set_device(device)\n",
        "    master_process = ddp_rank == 0\n",
        "else:\n",
        "    ddp_rank = 0\n",
        "    ddp_local_rank = 0\n",
        "    ddp_world_size = 1\n",
        "    master_process = True\n",
        "    device = \"cpu\"\n",
        "    if torch.cuda.is_available():\n",
        "        device = \"cuda\"\n",
        "    elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
        "        device = \"mps\"\n",
        "    print(f\"using device: {device}\")\n",
        "\n",
        "device_type = \"cuda\" if device.startswith(\"cuda\") else \"cpu\"\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(1337)\n",
        "\n",
        "enc = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "total_batch_size = 524288\n",
        "B = 32\n",
        "# B = 2\n",
        "T = 1024\n",
        "assert total_batch_size % (B * T * ddp_world_size) == 0, \"make sure total_batch_size is divisible by B * T * ddp_world_size\"\n",
        "grad_accum_steps = total_batch_size // (B * T * ddp_world_size)\n",
        "if master_process:\n",
        "    print(f\"total desired batch size: {total_batch_size}\")\n",
        "    print(f\"=> calculated gradient accumulation steps: {grad_accum_steps}\")\n",
        "\n",
        "train_loader = DataLoaderLite(B=B, T=T, process_rank=ddp_rank, num_processes=ddp_world_size, split=\"train\")\n",
        "val_loader = DataLoaderLite(B=B, T=T, process_rank=ddp_rank, num_processes=ddp_world_size, split=\"val\")\n",
        "\n",
        "torch.set_float32_matmul_precision('high')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 897
        },
        "id": "KvVyiIDprldF",
        "outputId": "19e675ce-bc39-43a0-a1e0-9be802bec4e4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-41-2018d2e27423>:11: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 1 loss 10.999573 | Duration: 0.8661 seconds\n",
            "Step 2 loss 9.917404 | Duration: 0.8082 seconds\n",
            "Step 3 loss 9.520599 | Duration: 0.8066 seconds\n",
            "Step 4 loss 9.314896 | Duration: 0.8038 seconds\n",
            "Step 5 loss 9.124435 | Duration: 0.8084 seconds\n",
            "Step 6 loss 8.765011 | Duration: 0.8172 seconds\n",
            "Step 7 loss 8.949883 | Duration: 0.8162 seconds\n",
            "Step 8 loss 8.822571 | Duration: 0.8165 seconds\n",
            "Step 9 loss 8.450050 | Duration: 0.8178 seconds\n",
            "Step 10 loss 9.753695 | Duration: 0.8205 seconds\n",
            "Step 11 loss 8.065765 | Duration: 0.8226 seconds\n",
            "Step 12 loss 7.982419 | Duration: 0.8209 seconds\n",
            "Step 13 loss 7.748437 | Duration: 0.8299 seconds\n",
            "Step 14 loss 7.354215 | Duration: 0.8277 seconds\n",
            "Step 15 loss 7.290989 | Duration: 0.8301 seconds\n",
            "Step 16 loss 7.034863 | Duration: 0.8360 seconds\n",
            "Step 17 loss 6.662466 | Duration: 0.8384 seconds\n",
            "Step 18 loss 6.520720 | Duration: 0.8427 seconds\n",
            "Step 19 loss 6.236246 | Duration: 0.8509 seconds\n",
            "Step 20 loss 5.964995 | Duration: 0.8448 seconds\n",
            "Step 21 loss 5.787529 | Duration: 0.8380 seconds\n",
            "Step 22 loss 5.508278 | Duration: 0.8511 seconds\n",
            "Step 23 loss 5.276625 | Duration: 0.8564 seconds\n",
            "Step 24 loss 5.067002 | Duration: 0.8622 seconds\n",
            "Step 25 loss 4.856206 | Duration: 0.8583 seconds\n",
            "Step 26 loss 4.612668 | Duration: 0.8668 seconds\n",
            "Step 27 loss 4.420388 | Duration: 0.8708 seconds\n",
            "Step 28 loss 4.200622 | Duration: 0.8737 seconds\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-2018d2e27423>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;31m# Forward pass with autocast for mixed precision (optional)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbfloat16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;31m# Backward pass with gradient scaling (optional for mixed precision)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-40-a1db5919fd22>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, idx, targets)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mblock\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mln_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-40-a1db5919fd22>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mln_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mln_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-40-a1db5919fd22>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_head\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_head\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_head\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_head\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaled_dot_product_attention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_causal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import time\n",
        "import torch\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "\n",
        "# Assuming you've already defined your model, optimizer, and data loader\n",
        "model = GPT(GPTConfig(vocab_size=50304))\n",
        "model = model.to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=6e-5, betas=(0.9, 0.999), eps=1e-8, weight_decay=0.01)\n",
        "\n",
        "# Initialize the scaler for mixed precision training (optional)\n",
        "scaler = GradScaler()\n",
        "\n",
        "# Load one batch (assuming you have a data loader named data_loader)\n",
        "x, y = train_loader.next_batch()\n",
        "x, y = x.to(device), y.to(device)\n",
        "\n",
        "# Start training loop\n",
        "start_time_2 = time.time()\n",
        "\n",
        "# Run 50 times to overfit the model on the single batch\n",
        "for i in range(50):\n",
        "    start_time = time.time()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Forward pass with autocast for mixed precision (optional)\n",
        "    with torch.autocast(device_type=device_type, dtype=torch.bfloat16):\n",
        "        logits, loss = model(x, y)\n",
        "\n",
        "    # Backward pass with gradient scaling (optional for mixed precision)\n",
        "    scaler.scale(loss).backward()\n",
        "    scaler.step(optimizer)\n",
        "    scaler.update()\n",
        "\n",
        "    # Sync GPU if using CUDA (optional but recommended for performance)\n",
        "    torch.cuda.synchronize()\n",
        "\n",
        "    # Record time per step\n",
        "    end_time = time.time()\n",
        "    step_duration = end_time - start_time\n",
        "    print(f\"Step {i+1} loss {loss.item():.6f} | Duration: {step_duration:.4f} seconds\")\n",
        "\n",
        "# After the loop, print the average step duration\n",
        "average_duration = (time.time() - start_time_2) / 50\n",
        "print(f\"Average step duration: {average_duration:.4f} seconds for 50 steps\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "u9reSbMLcjsI",
        "outputId": "0bad6224-3778-4542-c8a3-3eb273f7a9ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num decayed parameter tensors: 50, with 124,354,560 parameters\n",
            "num non-decayed parameter tensors: 98, with 121,344 parameters\n",
            "using fused AdamW: True\n",
            "validation loss: 10.9514\n",
            "step     0 | loss: 10.955009 | lr 5.0562e-06 | norm: 15.3464 | dt: 50681.09ms | tok/sec: 10344.84\n",
            "Model checkpoint saved at log/model_00000.pt\n",
            "step   100 | loss: 6.515263 | lr 5.1067e-04 | norm: 1.0224 | dt: 2706.43ms | tok/sec: 193719.17\n",
            "step   200 | loss: 5.832395 | lr 8.9995e-04 | norm: 0.4256 | dt: 2707.62ms | tok/sec: 193633.93\n",
            "validation loss: 5.5848\n",
            "step   300 | loss: 5.401945 | lr 8.9859e-04 | norm: 0.5298 | dt: 2709.02ms | tok/sec: 193534.32\n",
            "step   400 | loss: 4.857390 | lr 8.9533e-04 | norm: 0.5464 | dt: 2707.61ms | tok/sec: 193634.78\n",
            "validation loss: 4.5891\n",
            "step   500 | loss: 4.550300 | lr 8.9020e-04 | norm: 0.5809 | dt: 4108.06ms | tok/sec: 127624.25\n",
            "step   600 | loss: 4.154492 | lr 8.8322e-04 | norm: 0.4032 | dt: 2710.48ms | tok/sec: 193429.96\n",
            "step   700 | loss: 4.169420 | lr 8.7442e-04 | norm: 0.4605 | dt: 2710.26ms | tok/sec: 193445.45\n",
            "validation loss: 4.0704\n",
            "step   800 | loss: 4.073336 | lr 8.6385e-04 | norm: 0.3204 | dt: 2710.07ms | tok/sec: 193459.17\n",
            "step   900 | loss: 3.984676 | lr 8.5155e-04 | norm: 0.4528 | dt: 2710.11ms | tok/sec: 193456.63\n",
            "validation loss: 3.8416\n",
            "step  1000 | loss: 3.789350 | lr 8.3758e-04 | norm: 0.3107 | dt: 4106.60ms | tok/sec: 127669.61\n",
            "Model checkpoint saved at log/model_01000.pt\n",
            "step  1100 | loss: 3.674728 | lr 8.2200e-04 | norm: 0.3163 | dt: 2705.61ms | tok/sec: 193778.14\n",
            "step  1200 | loss: 3.591383 | lr 8.0489e-04 | norm: 0.3003 | dt: 2706.07ms | tok/sec: 193745.26\n",
            "validation loss: 3.7192\n",
            "step  1300 | loss: 3.667581 | lr 7.8633e-04 | norm: 0.2770 | dt: 2709.26ms | tok/sec: 193517.19\n",
            "step  1400 | loss: 3.752133 | lr 7.6641e-04 | norm: 0.2963 | dt: 2709.40ms | tok/sec: 193507.00\n",
            "validation loss: 3.6254\n",
            "step  1500 | loss: 3.687134 | lr 7.4522e-04 | norm: 0.3299 | dt: 4094.22ms | tok/sec: 128055.67\n",
            "step  1600 | loss: 3.720565 | lr 7.2285e-04 | norm: 0.3370 | dt: 2708.66ms | tok/sec: 193559.72\n",
            "step  1700 | loss: 3.549440 | lr 6.9942e-04 | norm: 0.2433 | dt: 2706.60ms | tok/sec: 193707.41\n",
            "validation loss: 3.5637\n",
            "step  1800 | loss: 3.469413 | lr 6.7503e-04 | norm: 0.3020 | dt: 2705.64ms | tok/sec: 193776.01\n",
            "step  1900 | loss: 3.434795 | lr 6.4979e-04 | norm: 0.2808 | dt: 2701.34ms | tok/sec: 194084.42\n",
            "validation loss: 3.5098\n",
            "step  2000 | loss: 3.549697 | lr 6.2384e-04 | norm: 0.3052 | dt: 4097.88ms | tok/sec: 127941.21\n",
            "Model checkpoint saved at log/model_02000.pt\n",
            "step  2100 | loss: 3.553772 | lr 5.9728e-04 | norm: 0.3371 | dt: 2706.26ms | tok/sec: 193731.32\n",
            "step  2200 | loss: 3.464810 | lr 5.7024e-04 | norm: 0.2669 | dt: 2708.88ms | tok/sec: 193544.28\n",
            "validation loss: 3.4595\n",
            "step  2300 | loss: 3.494261 | lr 5.4284e-04 | norm: 0.2625 | dt: 2706.27ms | tok/sec: 193730.69\n",
            "step  2400 | loss: 3.374846 | lr 5.1523e-04 | norm: 0.2452 | dt: 2708.40ms | tok/sec: 193578.75\n",
            "validation loss: 3.4267\n",
            "step  2500 | loss: 3.213350 | lr 4.8752e-04 | norm: 0.2839 | dt: 4095.64ms | tok/sec: 128011.29\n",
            "step  2600 | loss: 3.237136 | lr 4.5984e-04 | norm: 0.2665 | dt: 2706.03ms | tok/sec: 193748.39\n",
            "step  2700 | loss: 3.377400 | lr 4.3233e-04 | norm: 0.2369 | dt: 2705.23ms | tok/sec: 193805.49\n",
            "validation loss: 3.3900\n",
            "step  2800 | loss: 3.438440 | lr 4.0511e-04 | norm: 0.2861 | dt: 2710.22ms | tok/sec: 193448.75\n",
            "step  2900 | loss: 3.428305 | lr 3.7831e-04 | norm: 0.2671 | dt: 2710.52ms | tok/sec: 193426.95\n",
            "validation loss: 3.3597\n",
            "step  3000 | loss: 3.419369 | lr 3.5206e-04 | norm: 0.2754 | dt: 4123.51ms | tok/sec: 127146.07\n",
            "Model checkpoint saved at log/model_03000.pt\n",
            "step  3100 | loss: 3.429381 | lr 3.2648e-04 | norm: 0.2931 | dt: 2709.98ms | tok/sec: 193465.53\n",
            "step  3200 | loss: 3.309710 | lr 3.0169e-04 | norm: 0.2482 | dt: 2709.29ms | tok/sec: 193515.11\n",
            "validation loss: 3.3367\n",
            "step  3300 | loss: 3.267651 | lr 2.7780e-04 | norm: 0.2438 | dt: 2705.33ms | tok/sec: 193798.16\n",
            "step  3400 | loss: 3.187706 | lr 2.5493e-04 | norm: 0.2315 | dt: 2706.48ms | tok/sec: 193716.08\n",
            "validation loss: 3.3150\n",
            "step  3500 | loss: 3.383499 | lr 2.3319e-04 | norm: 0.2570 | dt: 4092.14ms | tok/sec: 128120.66\n",
            "step  3600 | loss: 3.359660 | lr 2.1267e-04 | norm: 0.2405 | dt: 2710.54ms | tok/sec: 193425.41\n",
            "step  3700 | loss: 3.233833 | lr 1.9347e-04 | norm: 0.7064 | dt: 2708.03ms | tok/sec: 193605.29\n",
            "validation loss: 3.2923\n",
            "step  3800 | loss: 3.287187 | lr 1.7569e-04 | norm: 0.2840 | dt: 2708.28ms | tok/sec: 193587.10\n",
            "step  3900 | loss: 3.316314 | lr 1.5939e-04 | norm: 0.2448 | dt: 2709.12ms | tok/sec: 193527.13\n",
            "validation loss: 3.2760\n",
            "step  4000 | loss: 3.246179 | lr 1.4468e-04 | norm: 0.2135 | dt: 4094.08ms | tok/sec: 128060.07\n",
            "Model checkpoint saved at log/model_04000.pt\n",
            "step  4100 | loss: 3.132488 | lr 1.3160e-04 | norm: 0.2159 | dt: 2709.11ms | tok/sec: 193527.68\n",
            "step  4200 | loss: 3.052162 | lr 1.2022e-04 | norm: 0.2117 | dt: 2699.93ms | tok/sec: 194185.52\n",
            "validation loss: 3.2659\n",
            "step  4300 | loss: 3.316530 | lr 1.1060e-04 | norm: 0.2906 | dt: 2706.37ms | tok/sec: 193723.53\n",
            "step  4400 | loss: 3.266829 | lr 1.0278e-04 | norm: 0.2297 | dt: 2703.08ms | tok/sec: 193959.30\n",
            "validation loss: 3.2550\n",
            "step  4500 | loss: 3.293002 | lr 9.6794e-05 | norm: 0.2387 | dt: 4099.53ms | tok/sec: 127889.69\n",
            "step  4600 | loss: 3.226579 | lr 9.2674e-05 | norm: 0.2350 | dt: 2705.94ms | tok/sec: 193754.26\n",
            "step  4700 | loss: 3.219124 | lr 9.0439e-05 | norm: 0.2282 | dt: 2709.24ms | tok/sec: 193518.53\n",
            "validation loss: 3.2479\n",
            "validation loss: 3.2469\n",
            "step  4800 | loss: 3.062030 | lr 9.0000e-05 | norm: 0.2473 | dt: 2709.94ms | tok/sec: 193468.36\n",
            "step  4900 | loss: 3.065628 | lr 9.0000e-05 | norm: 0.2284 | dt: 2707.02ms | tok/sec: 193676.89\n",
            "validation loss: 3.2435\n",
            "step  5000 | loss: 3.283789 | lr 9.0000e-05 | norm: 0.2499 | dt: 4826.46ms | tok/sec: 108627.85\n",
            "Model checkpoint saved at log/model_05000.pt\n",
            "step  5100 | loss: 3.329846 | lr 9.0000e-05 | norm: 0.2666 | dt: 2708.13ms | tok/sec: 193597.77\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-3b26655e093c>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mgrad_accum_steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0mloss_accum\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mddp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0mdist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_reduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_accum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mReduceOp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAVG\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    579\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m             )\n\u001b[0;32m--> 581\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    582\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 825\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    826\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "model = GPT(GPTConfig(vocab_size=50304))\n",
        "model.to(device)\n",
        "use_compile = True\n",
        "if use_compile:\n",
        "    model = torch.compile(model)\n",
        "if ddp:\n",
        "    model = DDP(model, device_ids=[ddp_local_rank])\n",
        "raw_model = model.module if ddp else model\n",
        "\n",
        "max_lr = 9e-4\n",
        "min_lr = max_lr * 0.1\n",
        "warmup_steps = int(715 / 4)\n",
        "max_steps = int(19073 / 4)\n",
        "def get_lr(it):\n",
        "    if it < warmup_steps:\n",
        "        return max_lr * (it+1) / warmup_steps\n",
        "    if it > max_steps:\n",
        "        return min_lr\n",
        "    decay_ratio = (it - warmup_steps) / (max_steps - warmup_steps)\n",
        "    assert 0 <= decay_ratio <= 1\n",
        "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))\n",
        "    return min_lr + coeff * (max_lr - min_lr)\n",
        "\n",
        "optimizer = raw_model.configure_optimizers(weight_decay=0.1, learning_rate=6e-4, device_type=device_type)\n",
        "save_interval = 1000\n",
        "\n",
        "log_dir = \"log\"\n",
        "os.makedirs(log_dir, exist_ok=True)\n",
        "log_file = os.path.join(log_dir, f\"log.txt\")\n",
        "with open(log_file, \"w\") as f:\n",
        "    pass\n",
        "\n",
        "for step in range(int(max_steps * 3)):\n",
        "    t0 = time.time()\n",
        "    last_step = (step == max_steps - 1)\n",
        "\n",
        "    if step % 250 == 0 or last_step:\n",
        "        model.eval()\n",
        "        val_loader.reset()\n",
        "        with torch.no_grad():\n",
        "            val_loss_accum = 0.0\n",
        "            val_loss_steps = 20\n",
        "            for _ in range(val_loss_steps):\n",
        "                x, y = val_loader.next_batch()\n",
        "                x, y = x.to(device), y.to(device)\n",
        "                with torch.autocast(device_type=device_type, dtype=torch.bfloat16):\n",
        "                    logits, loss = model(x, y)\n",
        "                loss = loss / val_loss_steps\n",
        "                val_loss_accum += loss.detach()\n",
        "        if ddp:\n",
        "            dist.all_reduce(val_loss_accum, op=dist.ReduceOp.AVG)\n",
        "        if master_process:\n",
        "            print(f\"validation loss: {val_loss_accum.item():.4f}\")\n",
        "            with open(log_file, \"a\") as f:\n",
        "                f.write(f\"{step} val {val_loss_accum.item():.4f}\\n\")\n",
        "            if step > 0 and (step % 5000 == 0 or last_step):\n",
        "                checkpoint_path = os.path.join(log_dir, f\"model_{step:05d}.pt\")\n",
        "                checkpoint = {\n",
        "                    'model': raw_model.state_dict(),\n",
        "                    'config': raw_model.config,\n",
        "                    'step': step,\n",
        "                    'val_loss': val_loss_accum.item()\n",
        "                }\n",
        "                # you might also want to add optimizer.state_dict() and\n",
        "                # rng seeds etc., if you wanted to more exactly resume training\n",
        "                torch.save(checkpoint, checkpoint_path)\n",
        "\n",
        "    # once in a while evaluate hellaswag\n",
        "    # if (step % 250 == 0 or last_step) and (not use_compile):\n",
        "    #     num_correct_norm = 0\n",
        "    #     num_total = 0\n",
        "    #     for i, example in enumerate(iterate_examples(\"val\")):\n",
        "    #         # only process examples where i % ddp_world_size == ddp_rank\n",
        "    #         if i % ddp_world_size != ddp_rank:\n",
        "    #             continue\n",
        "    #         # render the example into tokens and labels\n",
        "    #         _, tokens, mask, label = render_example(example)\n",
        "    #         tokens = tokens.to(device)\n",
        "    #         mask = mask.to(device)\n",
        "    #         # get the logits\n",
        "    #         with torch.no_grad():\n",
        "    #             with torch.autocast(device_type=device_type, dtype=torch.bfloat16):\n",
        "    #                 logits, loss = model(tokens)\n",
        "    #             pred_norm = get_most_likely_row(tokens, mask, logits)\n",
        "    #         num_total += 1\n",
        "    #         num_correct_norm += int(pred_norm == label)\n",
        "    #     # reduce the stats across all processes\n",
        "    #     if ddp:\n",
        "    #         num_total = torch.tensor(num_total, dtype=torch.long, device=device)\n",
        "    #         num_correct_norm = torch.tensor(num_correct_norm, dtype=torch.long, device=device)\n",
        "    #         dist.all_reduce(num_total, op=dist.ReduceOp.SUM)\n",
        "    #         dist.all_reduce(num_correct_norm, op=dist.ReduceOp.SUM)\n",
        "    #         num_total = num_total.item()\n",
        "    #         num_correct_norm = num_correct_norm.item()\n",
        "    #     acc_norm = num_correct_norm / num_total\n",
        "    #     if master_process:\n",
        "    #         print(f\"HellaSwag accuracy: {num_correct_norm}/{num_total}={acc_norm:.4f}\")\n",
        "    #         with open(log_file, \"a\") as f:\n",
        "    #             f.write(f\"{step} hella {acc_norm:.4f}\\n\")\n",
        "\n",
        "    # once in a while generate from the model (except step 0, which is noise)\n",
        "    # if ((step > 0 and step % 250 == 0) or last_step) and (not use_compile):\n",
        "    #     model.eval()\n",
        "    #     num_return_sequences = 4\n",
        "    #     max_length = 32\n",
        "    #     tokens = enc.encode(\"Hello, I'm a language model,\")\n",
        "    #     tokens = torch.tensor(tokens, dtype=torch.long)\n",
        "    #     tokens = tokens.unsqueeze(0).repeat(num_return_sequences, 1)\n",
        "    #     xgen = tokens.to(device)\n",
        "    #     sample_rng = torch.Generator(device=device)\n",
        "    #     sample_rng.manual_seed(42 + ddp_rank)\n",
        "    #     while xgen.size(1) < max_length:\n",
        "    #         # forward the model to get the logits\n",
        "    #         with torch.no_grad():\n",
        "    #             with torch.autocast(device_type=device_type, dtype=torch.bfloat16):\n",
        "    #                 logits, loss = model(xgen) # (B, T, vocab_size)\n",
        "    #             # take the logits at the last position\n",
        "    #             logits = logits[:, -1, :] # (B, vocab_size)\n",
        "    #             # get the probabilities\n",
        "    #             probs = F.softmax(logits, dim=-1)\n",
        "    #             # do top-k sampling of 50 (huggingface pipeline default)\n",
        "    #             # topk_probs here becomes (5, 50), topk_indices is (5, 50)\n",
        "    #             topk_probs, topk_indices = torch.topk(probs, 50, dim=-1)\n",
        "    #             # select a token from the top-k probabilities\n",
        "    #             # note: multinomial does not demand the input to sum to 1\n",
        "    #             ix = torch.multinomial(topk_probs, 1, generator=sample_rng) # (B, 1)\n",
        "    #             # gather the corresponding indices\n",
        "    #             xcol = torch.gather(topk_indices, -1, ix) # (B, 1)\n",
        "    #             # append to the sequence\n",
        "    #             xgen = torch.cat((xgen, xcol), dim=1)\n",
        "    #     # print the generated text\n",
        "    #     for i in range(num_return_sequences):\n",
        "    #         tokens = xgen[i, :max_length].tolist()\n",
        "    #         decoded = enc.decode(tokens)\n",
        "    #         print(f\"rank {ddp_rank} sample {i}: {decoded}\")\n",
        "\n",
        "    # do one step of the optimization\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    loss_accum = 0.0\n",
        "    for micro_step in range(grad_accum_steps):\n",
        "        x, y = train_loader.next_batch()\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        # added after video, this field is also used by the forward pass.\n",
        "        if ddp:\n",
        "            model.require_backward_grad_sync = (micro_step == grad_accum_steps - 1)\n",
        "        with torch.autocast(device_type=device_type, dtype=torch.bfloat16):\n",
        "            logits, loss = model(x, y)\n",
        "        # we have to scale the loss to account for gradient accumulation,\n",
        "        # because the gradients just add on each successive backward().\n",
        "        # addition of gradients corresponds to a SUM in the objective, but\n",
        "        # instead of a SUM we want MEAN. Scale the loss here so it comes out right\n",
        "        loss = loss / grad_accum_steps\n",
        "        loss_accum += loss.detach()\n",
        "        loss.backward()\n",
        "    if ddp:\n",
        "        dist.all_reduce(loss_accum, op=dist.ReduceOp.AVG)\n",
        "    norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "    # determine and set the learning rate for this iteration\n",
        "    lr = get_lr(step)\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "    optimizer.step()\n",
        "    if device_type == \"cuda\":\n",
        "        torch.cuda.synchronize() # wait for the GPU to finish work\n",
        "    t1 = time.time()\n",
        "    dt = t1 - t0 # time difference in seconds\n",
        "    tokens_processed = train_loader.B * train_loader.T * grad_accum_steps * ddp_world_size\n",
        "    tokens_per_sec = tokens_processed / dt\n",
        "    if master_process:\n",
        "        if step % 100 == 0:\n",
        "\n",
        "            print(f\"step {step:5d} | loss: {loss_accum.item():.6f} | lr {lr:.4e} | norm: {norm:.4f} | dt: {dt*1000:.2f}ms | tok/sec: {tokens_per_sec:.2f}\")\n",
        "        with open(log_file, \"a\") as f:\n",
        "            f.write(f\"{step} train {loss_accum.item():.6f}\\n\")\n",
        "\n",
        "        if step % save_interval == 0:\n",
        "            checkpoint_path = os.path.join(log_dir, f\"model_{step:05d}.pt\")\n",
        "            checkpoint = {\n",
        "                'model': raw_model.state_dict(),\n",
        "                'config': raw_model.config,\n",
        "                'step': step,\n",
        "                'optimizer': optimizer.state_dict(),\n",
        "            }\n",
        "            torch.save(checkpoint, checkpoint_path)\n",
        "            print(f\"Model checkpoint saved at {checkpoint_path}\")\n",
        "\n",
        "if ddp:\n",
        "    destroy_process_group()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def copy_to_drive():\n",
        "  google_drive_dir = \"/content/drive/My Drive/models/transformer/\"\n",
        "  os.makedirs(google_drive_dir, exist_ok=True)\n",
        "  local_output_dir = \"/content/log\"\n",
        "\n",
        "  for filename in os.listdir(local_output_dir):\n",
        "    local_file_path = os.path.join(local_output_dir, filename)\n",
        "    google_drive_file_path = os.path.join(google_drive_dir, filename)\n",
        "    shutil.copy(local_file_path, google_drive_file_path)\n",
        "    print(f\"Copied {filename} to Google Drive.\")\n",
        "\n",
        "  print(f\"All files copied to Google Drive at {google_drive_dir}\")\n",
        "copy_to_drive()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kpua0MKNAja9",
        "outputId": "d12fbeca-2dd1-4cbc-c1d3-6969aee39f5a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copied model_01000.pt to Google Drive.\n",
            "Copied log.txt to Google Drive.\n",
            "Copied model_00000.pt to Google Drive.\n",
            "Copied model_03000.pt to Google Drive.\n",
            "Copied model_05000.pt to Google Drive.\n",
            "Copied model_02000.pt to Google Drive.\n",
            "Copied model_04000.pt to Google Drive.\n",
            "Copied model_04767.pt to Google Drive.\n",
            "All files copied to Google Drive at /content/drive/My Drive/models/transformer/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "num_return_sequences = 4\n",
        "max_length = 32\n",
        "tokens = enc.encode(\"Hello, I'm a language model,\")\n",
        "tokens = torch.tensor(tokens, dtype=torch.long)\n",
        "tokens = tokens.unsqueeze(0).repeat(num_return_sequences, 1)\n",
        "xgen = tokens.to(device)\n",
        "sample_rng = torch.Generator(device=device)\n",
        "sample_rng.manual_seed(42 + ddp_rank)\n",
        "while xgen.size(1) < max_length:\n",
        "    # forward the model to get the logits\n",
        "    with torch.no_grad():\n",
        "        with torch.autocast(device_type=device_type, dtype=torch.bfloat16):\n",
        "            logits, loss = model(xgen) # (B, T, vocab_size)\n",
        "        # take the logits at the last position\n",
        "        logits = logits[:, -1, :] # (B, vocab_size)\n",
        "        # get the probabilities\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "        # do top-k sampling of 50 (huggingface pipeline default)\n",
        "        # topk_probs here becomes (5, 50), topk_indices is (5, 50)\n",
        "        topk_probs, topk_indices = torch.topk(probs, 50, dim=-1)\n",
        "        # select a token from the top-k probabilities\n",
        "        # note: multinomial does not demand the input to sum to 1\n",
        "        ix = torch.multinomial(topk_probs, 1, generator=sample_rng) # (B, 1)\n",
        "        # gather the corresponding indices\n",
        "        xcol = torch.gather(topk_indices, -1, ix) # (B, 1)\n",
        "        # append to the sequence\n",
        "        xgen = torch.cat((xgen, xcol), dim=1)\n",
        "# print the generated text\n",
        "for i in range(num_return_sequences):\n",
        "    tokens = xgen[i, :max_length].tolist()\n",
        "    decoded = enc.decode(tokens)\n",
        "    print(f\"rank {ddp_rank} sample {i}: {decoded}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cPg0QTFLBP5Q",
        "outputId": "4edcac95-0da5-4d11-d5e9-6a53596c9e0e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rank 0 sample 0: Hello, I'm a language model, and I don't want to go down as if I were to create a robot. It's the way the robot was\n",
            "rank 0 sample 1: Hello, I'm a language model, so i'm going to explain it back to the basics I've got all along, but this one actually needs some context\n",
            "rank 0 sample 2: Hello, I'm a language model, so I could see lots of fun with it - especially if your class is a computer language model.\n",
            "I've added\n",
            "rank 0 sample 3: Hello, I'm a language model, and now I'm going to show you some.\n",
            "First, let's go. The following code contains some code used\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "tgcAxZkhmiXj",
        "VYNfrDwvXKFY",
        "8vgyE77KyqGO"
      ],
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}